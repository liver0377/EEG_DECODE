{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from transformers import CLIPVisionModel \n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "# import os\n",
    "# proxy = 'http://127.0.0.1:7890'\n",
    "# os.environ['http_proxy'] = proxy\n",
    "# os.environ['https_proxy'] = proxy\n",
    "# import subprocess\n",
    "# import os\n",
    "\n",
    "# result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "# output = result.stdout\n",
    "# for line in output.splitlines():\n",
    "#     if '=' in line:\n",
    "#         var, value = line.split('=', 1)\n",
    "#         os.environ[var] = value\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import CLIPVisionModel\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPVisionModelWithProjection, CLIPImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.clip = CLIPVisionModel.from_pretrained('openai/clip-vit-large-patch14').to(torch.bfloat16)\n",
    "        # self.clip_size = (224, 224)\n",
    "\n",
    "        self.preprocess = CLIPImageProcessor(\n",
    "            size={\"height\": 512, \"width\": 512},\n",
    "            crop_size={\"height\": 512, \"width\": 512},\n",
    "        )\n",
    "\n",
    "\n",
    "        # for param in self.clip.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n",
    "        \"h94/IP-Adapter\", \n",
    "        # \"laion2b_s32b_b79k\",\n",
    "        subfolder=\"models/image_encoder\",\n",
    "        torch_dtype=torch.float16,\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        \n",
    "    \n",
    "def encode_image(image, image_encoder, feature_extractor, num_images_per_prompt=1, device='cuda'):\n",
    "    dtype = next(image_encoder.parameters()).dtype\n",
    "\n",
    "    if not isinstance(image, torch.Tensor):\n",
    "        image = feature_extractor(image, return_tensors=\"pt\").pixel_values # [1, 3, 224, 224]\n",
    "        print(\"image\", image.shape)\n",
    "    image = image.to(device=device, dtype=dtype)\n",
    "    image_embeds = image_encoder(image).image_embeds # (1, 1024)\n",
    "    image_embeds = image_embeds.repeat_interleave(num_images_per_prompt, dim=0) # (num_images_per_prompt, 1024)\n",
    "\n",
    "    return image_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_image_path = \"/root/autodl-tmp/Workspace/EEG_caption/docs/test/alpaca_03s.jpg\"\n",
    "clip_encoder = CLIPEncoder().to(device)\n",
    "\n",
    "# from diffusers.utils import load_image\n",
    "# image_prompt = load_image(example_image_path)\n",
    "# display(image_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.open(example_image_path).convert(\"RGB\")\n",
    "# image_embeds = encode_image(image_prompt, clip_encoder.image_encoder, clip_encoder.preprocess, 1, device)\n",
    "# image_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.save(image_embeds, 'image_embeds.pt')\n",
    "image_embeds_1 = torch.load('/root/autodl-tmp/BrainAligning/ViT-H-14_features_test.pt', map_location=device)['img_features'].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_embeds_1 = torch.load('/root/autodl-tmp/BrainAligning/ATM_S_eeg_features_sub-08_test.pt', map_location=device).unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_embeds_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_feature = img_feature.mean(dim=1).squeeze(0)\n",
    "# img_feature = img_feature.unsqueeze(0)\n",
    "# img_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_clip_img_feature = torch.load('ViT-H-14_features_test.pt', map_location=device)['img_features'].unsqueeze(1)\n",
    "# open_clip_img_feature[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_clip_img_feature[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# image_features_test = torch.load(\"/home/ldy/Workspace/EEG_caption/ViT-H-14_features_test.pt\")\n",
    "# image_features_test['img_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_features_test['img_features'][0].unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# image_features_test = torch.load(\"/home/ldy/Workspace/BrainAligning/ViT-L-14_features_multimodal_test.pt\")\n",
    "# image_features_test['img_features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "# import open_clip\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "\n",
    "from diffusion_prior import *\n",
    "from custom_pipeline_low_level import *\n",
    "# from custom_pipeline import *\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embeds_1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "index = 65\n",
    "low_level_img_path = f'/root/autodl-tmp/BrainAligning/vae_imgs/epoch_170/reconstructed_image_{index}_20.png'\n",
    "# Provides low-level images \n",
    "low_level_image = Image.open(low_level_img_path)\n",
    "low_level_image.show()\n",
    "low_level_image = clip_encoder.preprocess(low_level_image, return_tensors=\"pt\").pixel_values # [1, 3, 224, 224]\n",
    "low_level_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.image_processor import VaeImageProcessor\n",
    "image_processor = VaeImageProcessor()\n",
    "from diffusers import AutoencoderKL\n",
    "# path = \"stabilityai/sdxl-turbo\"\n",
    "# vlmodel = AutoencoderKL.from_pretrained(\n",
    "#     path, subfolder='vae').to(device)\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-xl-base-1.0\", torch_dtype=torch.float, variant=\"fp16\")\n",
    "vlmodel = pipe.vae\n",
    "\n",
    "posterior = vlmodel.encode(low_level_image).latent_dist\n",
    "image_latent = posterior.mode()\n",
    "print(\"image_latent.shape\", image_latent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     x_rec = vlmodel.decode(image_latent).sample\n",
    "#     image_rec = image_processor.postprocess(x_rec, output_type='pil')\n",
    "#     image_rec[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "diffusion_prior = DiffusionPriorUNet(cond_dim=1024, dropout=0.1)\n",
    "# number of parameters\n",
    "print(sum(p.numel() for p in diffusion_prior.parameters() if p.requires_grad))\n",
    "pipe = Pipe(diffusion_prior, device=device)\n",
    "\n",
    "# load pretrained model\n",
    "pipe.diffusion_prior.load_state_dict(torch.load(f'/root/autodl-tmp/BrainAligning/fintune_ckpts/diffusion_prior.pt', map_location=device))\n",
    "h = pipe.generate(c_embeds=eeg_embeds_1[index], num_inference_steps=10, guidance_scale=2.0)\n",
    "\n",
    "\n",
    "generator = Generator4Embeds(num_inference_steps=4, device=device)\n",
    "# set a seed value\n",
    "seed_value = 42\n",
    "gen = torch.Generator(device=device)\n",
    "gen.manual_seed(seed_value)\n",
    "\n",
    "image_1 = generator.generate(eeg_embeds_1[index], generator=gen)  \n",
    "display(image_1)\n",
    "\n",
    "image_2 = generator.generate(h, generator=gen)  \n",
    "display(image_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "generator = Generator4Embeds(num_inference_steps=10, device=device, img2img_strength=0.5, low_level_image=low_level_image, low_level_latent=None)\n",
    "# set a seed value\n",
    "# seed_value = 30\n",
    "gen = torch.Generator(device=device)\n",
    "gen.manual_seed(seed_value)\n",
    "image = generator.generate(h,            \n",
    "                           generator=gen)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "generator = Generator4Embeds(num_inference_steps=10, device=device, img2img_strength=0.5, low_level_image=low_level_image, low_level_latent=None)\n",
    "# set a seed value\n",
    "# seed_value = 30\n",
    "gen = torch.Generator(device=device)\n",
    "gen.manual_seed(seed_value)\n",
    "image = generator.generate(h,\n",
    "                           text_prompt = \"a white ceramic finish ceramic finish with a white ceramic finish.\",               \n",
    "                           generator=gen)\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed_value = 42\n",
    "# gen = torch.Generator(device=device)\n",
    "# gen.manual_seed(seed_value)\n",
    "# image = generator.generate(img_feature, generator=gen)\n",
    "# display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from IPython.display import display\n",
    "\n",
    "# Assume there's a class Generator4Embeds and a clip_encoder preprocessing method\n",
    "# You can modify these classes and methods based on your existing code\n",
    "\n",
    "# Original image directory and output image directory\n",
    "input_dir = '/root/autodl-tmp/BrainAligning/vae_imgs/epoch_170'\n",
    "output_dir = '/root/autodl-tmp/BrainAligning/reconstructed_imgs'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Set seed value\n",
    "seed_value = 42\n",
    "gen = torch.Generator(device=device)\n",
    "gen.manual_seed(seed_value)\n",
    "\n",
    "# Modify sorting function to extract the number after the first \"_\"\n",
    "def extract_number(file_name):\n",
    "    # Assume the file name format is \"reconstructed_image_0_2.png\"\n",
    "    return int(file_name.split('_')[2])  # Extract the number after the first \"_\"\n",
    "\n",
    "# Iterate through all image files in the directory and sort them by the number after the first \"_\"\n",
    "for i, file_name in enumerate(sorted(os.listdir(input_dir), key=extract_number)):\n",
    "    if i % 80 == 0:\n",
    "        print(file_name)            \n",
    "        if file_name.endswith(\".png\"):  # Only process PNG images\n",
    "            img_path = os.path.join(input_dir, file_name)\n",
    "            \n",
    "            # Open and preprocess the image\n",
    "            low_level_image = Image.open(img_path)\n",
    "            # low_level_image.show()\n",
    "            low_level_image = clip_encoder.preprocess(low_level_image, return_tensors=\"pt\").pixel_values  # [1, 3, 224, 224]\n",
    "            \n",
    "            # Create an instance of the generator\n",
    "            generator = Generator4Embeds(num_inference_steps=5, device=device, img2img_strength=0.8, low_level_image=low_level_image)\n",
    "            \n",
    "            # Generate the reconstructed image\n",
    "            reconstructed_image = generator.generate(eeg_embeds_1[i // 80], generator=gen)\n",
    "            \n",
    "            # Save the reconstructed image to the new directory\n",
    "            output_path = os.path.join(output_dir, file_name)\n",
    "            reconstructed_image.save(output_path)\n",
    "            \n",
    "            # Optionally display the generated image\n",
    "            # display(reconstructed_image)\n",
    "\n",
    "print(\"All images have been successfully reconstructed and saved to:\", output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "umbrae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
